---
title: Random Forests
authors:
- Leo Breiman
date: '2001-01-01'
publishDate: '2024-06-05T20:56:27.385844Z'
publication_types:
- article-journal
publication: '*Machine Learning*'
doi: 10.1023/A:1010933404324
abstract: 'Random forests are a combination of tree predictors such that each tree
  depends on the values of a random vector sampled independently and with the same
  distribution for all trees in the forest. The generalization error for forests converges
  a.s. to a limit as the number of trees in the forest becomes large. The generalization
  error of a forest of tree classifiers depends on the strength of the individual
  trees in the forest and the correlation between them. Using a random selection of
  features to split each node yields error rates that compare favorably to Adaboost
  (Y. Freund &amp; R. Schapire, Machine Learning: Proceedings of the Thirteenth International
  conference, ***, 148â€“156), but are more robust with respect to noise. Internal estimates
  monitor error, strength, and correlation and these are used to show the response
  to increasing the number of features used in the splitting. Internal estimates are
  also used to measure variable importance. These ideas are also applicable to regression.'
tags:
- Artificial Intelligence
- Control
- Mechatronics
- Natural Language Processing (NLP)
- Robotics
- Simulation and Modeling
links:
- name: URL
  url: http://link.springer.com/10.1023/A:1010933404324
---
